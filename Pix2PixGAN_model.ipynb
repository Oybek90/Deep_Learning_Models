{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24050\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import normalize\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot \n",
    "\n",
    "\n",
    "#Resizing images, if needed\n",
    "SIZE_X = 64\n",
    "SIZE_Y = 64\n",
    "\n",
    "#Capture training image info as a list\n",
    "tar_images = []\n",
    "root = \"E:/UPWORK/GAN_paper/image slice-T1/\"\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        img = cv2.imread(os.path.join(path, name), 1)       \n",
    "        img = cv2.resize(img, dsize=(SIZE_Y, SIZE_X))\n",
    "        tar_images.append(img)\n",
    "        \n",
    "print(len(tar_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24050, 64, 64, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(tar_images)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('med_slice_T1_64.pickle', 'wb') as f:\n",
    "    pickle.dump(tar_images, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/UcHe0xiuvpg\n",
    "# https://youtu.be/6pUSZgPJ3Yg\n",
    "# https://youtu.be/my7LEgYTJto\n",
    "\"\"\"\n",
    "pix2pix GAN model\n",
    "Based on the code by Jason Brownlee from his blogs on https://machinelearningmastery.com/\n",
    "I seriously urge everyone to foloow his blogs and get enlightened. \n",
    "I am adapting his code to various applications but original credit goes to Jason.\n",
    "    Original paper: https://arxiv.org/pdf/1611.07004.pdf\n",
    "    Github for original paper: https://phillipi.github.io/pix2pix/\n",
    "    \n",
    "Generator:    \n",
    "The encoder-decoder architecture consists of:\n",
    "encoder:\n",
    "C64-C128-C256-C512-C512-C512-C512-C512\n",
    "decoder:\n",
    "CD512-CD512-CD512-C512-C256-C128-C64\n",
    "Discriminator\n",
    "C64-C128-C256-C512\n",
    "After the last layer, a convolution is applied to map to\n",
    "a 1-dimensional output, followed by a Sigmoid function.    \n",
    "\"\"\"\n",
    "# \n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#Define generator, discriminator, gan and other helper functions\n",
    "#We will use functional way of defining model and not sequential\n",
    "#as we have multiple inputs; both images and corresponding labels. \n",
    "########################################################################\n",
    "\n",
    "#Since pix2pix is a conditional GAN, it takes 2 inputs - image and corresponding label\n",
    "#For pix2pix the label will be another image. \n",
    "\n",
    "# define the standalone discriminator model\n",
    "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
    "    #Binary classification - true or false (1 or 0). So using sigmoid activation.\n",
    "#Think of discriminator as a binary classifier that is classifying images as real/fake.\n",
    "\n",
    "# From the paper C64-C128-C256-C512\n",
    "#After the last layer, conv to 1-dimensional output, followed by a Sigmoid function.  \n",
    "\n",
    "def define_discriminator(image_shape):\n",
    "    \n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02) #As described in the original paper\n",
    "    \n",
    "\t# source image input\n",
    "\tin_src_image = Input(shape=image_shape)  #Image we want to convert to another image\n",
    "\t# target image input\n",
    "\tin_target_image = Input(shape=image_shape)  #Image we want to generate after training. \n",
    "    \n",
    "\t# concatenate images, channel-wise\n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "    \n",
    "\t# C64: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256: 4x4 kernel Stride 2x2\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512: 4x4 kernel Stride 2x2 \n",
    "    # Not in the original paper. Comment this block if you want.\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer : 4x4 kernel but Stride 1x1\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
    "\t# compile model\n",
    "    #The model is trained with a batch size of one image and Adam opt. \n",
    "    #with a small learning rate and 0.5 beta. \n",
    "    #The loss for the discriminator is weighted by 50% for each model update.\n",
    "    \n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\treturn model\n",
    "\n",
    "# disc_model = define_discriminator((256,256,3))\n",
    "# plot_model(disc_model, to_file='disc_model.png', show_shapes=True)\n",
    "\n",
    "##############################\n",
    "#Now define the generator - in our case we will define a U-net\n",
    "# define an encoder block to be used in generator\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# define a decoder block to be used in generator\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model - U-net\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "\te2 = define_encoder_block(e1, 128)\n",
    "\te3 = define_encoder_block(e2, 256)\n",
    "\te4 = define_encoder_block(e3, 512)\n",
    "\te5 = define_encoder_block(e4, 512)\n",
    "\te6 = define_encoder_block(e5, 512)\n",
    "\te7 = define_encoder_block(e6, 512)\n",
    "\t# bottleneck, no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "\tb = Activation('relu')(b)\n",
    "\t# decoder model: CD512-CD512-CD512-C512-C256-C128-C64\n",
    "\td1 = decoder_block(b, e7, 512)\n",
    "\td2 = decoder_block(d1, e6, 512)\n",
    "\td3 = decoder_block(d2, e5, 512)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\t# output\n",
    "\tg = Conv2DTranspose(image_shape[2], (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) #Modified \n",
    "\tout_image = Activation('tanh')(g)  #Generates images in the range -1 to 1. So change inputs also to -1 to 1\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model\n",
    "\n",
    "# gen_model = define_generator((256,256,3))\n",
    "# plot_model(gen_model, to_file='gen_model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but \n",
    "                                                #standalone descriminator will be trainable.\n",
    "            \n",
    "\t# define the source image\n",
    "\tin_src = Input(shape=image_shape)\n",
    "\t# suppy the image as input to the generator \n",
    "\tgen_out = g_model(in_src)\n",
    "\t# supply the input image and generated image as inputs to the discriminator\n",
    "\tdis_out = d_model([in_src, gen_out])\n",
    "\t# src image as input, generated image and disc. output as outputs\n",
    "\tmodel = Model(in_src, [dis_out, gen_out])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    \n",
    "    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE)\n",
    "    #Authors suggested weighting BCE vs L1 as 1:100.\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'mae'], \n",
    "               optimizer=opt, loss_weights=[1,100])\n",
    "\treturn model\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2 = trainA[ix], trainB[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn [X1, X2], y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(samples)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "#GAN models do not converge, we just want to find a good balance between\n",
    "#the generator and the discriminator. Therefore, it makes sense to periodically\n",
    "#save the generator model and check how good the generated image looks. \n",
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "\t# select a sample of input images\n",
    "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "\t# generate a batch of fake samples\n",
    "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_realA = (X_realA + 1) / 2.0\n",
    "\tX_realB = (X_realB + 1) / 2.0\n",
    "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
    "\t# plot real source images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_realA[i])\n",
    "\t# plot generated target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_fakeB[i])\n",
    "\t# plot real target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_realB[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
    "\tplt.savefig(filename1)\n",
    "\tplt.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "# train pix2pix models\n",
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=50):\n",
    "\t# determine the output square shape of the discriminator\n",
    "\tn_patch = d_model.output_shape[1]\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\t\t# update discriminator for real samples\n",
    "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "\t\t# update discriminator for generated samples\n",
    "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "\t\t# update the generator\n",
    "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\t\t# summarize model performance\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
